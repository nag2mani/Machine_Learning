{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  Assignment 1\n",
    "You are given $n$ documents (labels $\\in \\{-1,+1\\}$), each consisting of $n_i$ words from some finite vocabulary $\\mathsf{V=\\{v_1,v_2,\\dots,v_d\\}}$ with size $d$.\n",
    "We represent document $i$ by a vector \n",
    "$$\n",
    "    \\mathsf{\\mathbf{x_i} = \\begin{pmatrix} \\mathsf{x_{i1}, x_{i2},\\dots,x_{id},1 }\\end{pmatrix}}\n",
    "$$ where $x_{ij}=$ number of times that word $v_j$ appears in document $i$.\n",
    "Now let matrix $X$ of size $n$ by $d+1$ where its $i$-th row is equal to $\\mathbf{x_i}$; i.e.\n",
    "$$\n",
    "    \\mathsf{X = \\begin{pmatrix} \\mathsf{x_{ij}} \\end{pmatrix}_{i\\in[n],j\\in[d+1]}}\n",
    "$$\n",
    "also let vector $\\mathbf{y} \\in \\{-1,+1\\}^n$ be\n",
    "$$\n",
    "    \\mathbf{y}=\\begin{pmatrix} \\mathsf{y_1 \\\\\n",
    "                                   y_2 \\\\\n",
    "                                   \\dots \\\\\n",
    "                                   y_n}\n",
    "                                   \\end{pmatrix}\n",
    "$$\n",
    "wher $y_i$ is the label of document $i$.\n",
    "### Buidling $X$ and $\\mathbf{y}$\n",
    "You are given two text files\n",
    "1. `train.txt`: training set (labeled documents)\n",
    "2. `test.txt`: test set (unlabeled documents)\n",
    "\n",
    "in `train.txt`:\n",
    "- line $1$ is the number of documents in training set $n_{\\mathrm{train}}$ \n",
    "- line $2i$ contain words in document $i$ separated by space  ($i=1,2,\\dots,n_{\\mathrm{train}}$)\n",
    "- line $2i+1$ contain label for document $i$    ($i=1,2,\\dots,n_{\\mathrm{train}}$)\n",
    "\n",
    "in `test.txt`:\n",
    "- line $1$ is the number of documents in training set $n_{\\mathrm{test}}$ \n",
    "- line $i+1$ contain words in document $i$ separated by space  ($i=1,2,\\dots,n_{\\mathrm{test}}$)\n",
    "\n",
    "Now let's build $X_\\mathrm{train}$,$X_\\mathrm{test}$, and $\\mathbf{y}_{\\mathrm{train}}$ as defined above using `train.txt` and `test.txt`\n",
    "\n",
    "### IMPORTANT\n",
    "- Documents are numbered as they appear in the input file\n",
    "- let Vocabulary $V=(v_1,v_2,\\dots,v_d)$ be **sorted** list of all distinct word in the documents of **training** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (ACT1-3) using input files compute the following\n",
    "docs_train = ACT1 #list of documents where each document is a list of words\n",
    "docs_test = ACT2 #list of documents where each document is a list of words\n",
    "labels = ACT3   #list of labels each either -1 or +1 \n",
    "\n",
    "\n",
    "n_train = len(docs_train)\n",
    "n_test = len(docs_test)\n",
    "print(docs_train[40])\n",
    "print(docs_test[40])\n",
    "\n",
    "#CHECKS\n",
    "assert((n_train+n_test)==1500)\n",
    "assert(np.sum(np.array(labels))== -348)\n",
    "assert(len(docs_train[1])+len(docs_test[1])==12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT4) design a function that takes list of documents (list of list of words) \n",
    "# as input and returns sorted list of distinct words \n",
    "# use built-in sort in python for sorting strings\n",
    "def make_vocabulary(docs):\n",
    "    ACT4\n",
    "    \n",
    "vocab = make_vocabulary(docs_train)\n",
    "d = len(vocab) \n",
    "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
    "print(vocab)\n",
    "print(w2i)\n",
    "\n",
    "#CHECKS\n",
    "assert(vocab[2]==\"LnGi\")\n",
    "assert(vocab == sorted(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT5) design a function that takes \n",
    "# (1) docs: list of documents (i.e. list of list of words)\n",
    "# (2) w2i: a dictionary that maps words to index\n",
    "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
    "# (DO NOT forget last column of X which is all 1)\n",
    "\n",
    "def make_matrix(docs, w2i):\n",
    "    ACT5\n",
    "\n",
    "X_train = make_matrix(docs_train,w2i)\n",
    "X_test = make_matrix(docs_test,w2i)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# (ACT6-8)\n",
    "print (\"First 5 rows of X_train: \")\n",
    "print(ACT6)\n",
    "print (\"Last 5 rows of X_test: \")\n",
    "print(ACT7)\n",
    "print (\"First 10 labels of training set:\")\n",
    "print (ACT8)\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(X_train)==6871)\n",
    "assert(np.sum(X_test)==3462)\n",
    "assert(np.sum(X_test[10,:]+X_train[10,:])==11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Predictor\n",
    "\n",
    "Let $\\mathbf{w} \\in \\mathbb{R}^{d+1}$, for a single document $\\mathbf{x} \\in \\mathbb{R}^{d+1}$, our predicted label is\n",
    "$$\n",
    "    \\mathsf{\\hat{y} = \\mathrm{sign}({\\mathbf{w} \\cdot \\mathbf{y}})}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "    \\displaystyle  \\mathrm{sign}(\\alpha) = \\begin{cases} \n",
    "      +1 & \\alpha \\geq 0 \\\\\n",
    "      -1 & \\alpha < 0 \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, for documents matrix $X \\in \\mathbb{R}^{n\\times(d+1)}$ we predict labels $\\hat{\\mathbf{y}} \\in \\{-1,+1\\}^n$\n",
    "$$\n",
    "    \\hat{\\mathbf{y}} = \\mathrm{sign}(X\\mathbf{w})\n",
    "$$\n",
    "where $\\mathrm{sign()}$ here is defined to elementwise applying the sign we defined previously.\n",
    "\n",
    "The error of our prediction over $X$ with true labels $\\mathbf{y}$, is defined to be\n",
    "$$\n",
    "    \\mathsf{\\mathrm{error}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{01}(y_i,\\hat{y}_i)}\n",
    "$$\n",
    "where $\\displaystyle \\mathsf{ \\ell_{01}(y,\\hat{y})= \\begin{cases} 1 & y\\neq\\hat{y} \\\\ 0 & \\text{otherwise}\\end{cases} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
    "# and computes the error\n",
    "def err(y,y_hat):\n",
    "    ACT9\n",
    "\n",
    "#CHECKS\n",
    "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
    "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
    "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT10) Design a function that takes as input\n",
    "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
    "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
    "# and output \n",
    "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
    "def predict(X,w):\n",
    "    ACT10\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate list for $\\mathbf{w}$\n",
    "we give you a small candidates list `candids` of $\\mathbf{w}$'s. We want you to find $\\mathbf{w}^*$ in this list which gives you the smallest error over **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading candidates list candids = [w0,w1,...]\n",
    "import pickle\n",
    "with open('candids.pkl', 'rb') as f:\n",
    "    candids = pickle.load(f)\n",
    "print(\"size of candidates lists %d\"%len(candids))\n",
    "print(candids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT11) fill err_list with training error of each candidate w\n",
    "err_list = ACT11\n",
    "# (ACT12) index of w with smallest error over training set \n",
    "best_index = ACT12\n",
    "\n",
    "print(\"Training Error of candidates:\")\n",
    "for i,err in enumerate(err_list):\n",
    "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
    "\n",
    "print(\"Index of best predictor: %d\"%best_index)\n",
    "print(\"Best Predictor:\")\n",
    "print(candids[best_index])\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(err_list)<=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best predictor\n",
    "w_best = candids[best_index]\n",
    "\n",
    "# (ACT13) Use w_best to predict labels for X_test \n",
    "y_test = ACT13\n",
    "\n",
    "# (ACT14) print first 10 labels predicted for test set\n",
    "print(ACT14)\n",
    "\n",
    "#CHECKS\n",
    "def my_hash(y):\n",
    "    p1 = 28433\n",
    "    p2 = 577\n",
    "    ret = 0\n",
    "    for e in range(len(y)):\n",
    "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
    "    return ret\n",
    "assert(my_hash(y_test) == 19262)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT15) using X_train and X_test\n",
    "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
    "# using two subplots (bar plots) one for X_train and one for X_test\n",
    "# you might find plt.bar useful\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACT15\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
