{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  Assignment 1\n",
    "You are given $n$ documents (labels $\\in \\{-1,+1\\}$), each consisting of $n_i$ words from some finite vocabulary $\\mathsf{V=\\{v_1,v_2,\\dots,v_d\\}}$ with size $d$.\n",
    "We represent document $i$ by a vector \n",
    "$$\n",
    "    \\mathsf{\\mathbf{x_i} = \\begin{pmatrix} \\mathsf{x_{i1}, x_{i2},\\dots,x_{id},1 }\\end{pmatrix}}\n",
    "$$\n",
    "where $x_{ij}=$ number of times that word $v_j$ appears in document $i$.\n",
    "Now let matrix $X$ of size $n$ by $d+1$ where its $i$-th row is equal to $\\mathbf{x_i}$; i.e.\n",
    "$$\n",
    "    \\mathsf{X = \\begin{pmatrix} \\mathsf{x_{ij}} \\end{pmatrix}_{i\\in[n],j\\in[d+1]}}\n",
    "$$\n",
    "also let vector $\\mathbf{y} \\in \\{-1,+1\\}^n$ be\n",
    "$$\n",
    "    \\mathbf{y}=\\begin{pmatrix} \\mathsf{y_1 \\\\\n",
    "                                   y_2 \\\\\n",
    "                                   \\dots \\\\\n",
    "                                   y_n}\n",
    "                                   \\end{pmatrix}\n",
    "$$\n",
    "wher $y_i$ is the label of document $i$.\n",
    "### Buidling $X$ and $\\mathbf{y}$\n",
    "You are given two text files\n",
    "1. `train.txt`: training set (labeled documents)\n",
    "2. `test.txt`: test set (unlabeled documents)\n",
    "\n",
    "in `train.txt`:\n",
    "- line $1$ is the number of documents in training set $n_{\\mathrm{train}}$ \n",
    "- line $2i$ contain words in document $i$ separated by space  ($i=1,2,\\dots,n_{\\mathrm{train}}$)\n",
    "- line $2i+1$ contain label for document $i$    ($i=1,2,\\dots,n_{\\mathrm{train}}$)\n",
    "\n",
    "in `test.txt`:\n",
    "- line $1$ is the number of documents in training set $n_{\\mathrm{test}}$ \n",
    "- line $i+1$ contain words in document $i$ separated by space  ($i=1,2,\\dots,n_{\\mathrm{test}}$)\n",
    "\n",
    "Now let's build $X_\\mathrm{train}$,$X_\\mathrm{test}$, and $\\mathbf{y}_{\\mathrm{train}}$ as defined above using `train.txt` and `test.txt`\n",
    "\n",
    "### IMPORTANT\n",
    "- Documents are numbered as they appear in the input file\n",
    "- let Vocabulary $V=(v_1,v_2,\\dots,v_d)$ be **sorted** list of all distinct word in the documents of **training** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACT-1\n",
    "\n",
    "with open('train.txt', 'r') as file:\n",
    "    docs_train_list = [line.strip().split() for line in file if len(line) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACT-2\n",
    "\n",
    "with open('test.txt', 'r') as file:\n",
    "    docs_test_list = [line.strip().split() for line in file if len(line) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACT-3\n",
    "\n",
    "with open('train.txt', 'r') as file:\n",
    "    docs_train_list_levels = [line.strip().split() for line in file if len(line) <= 3]\n",
    "level = []\n",
    "for i in range(len(docs_train_list_levels)):\n",
    "    level.append(int(docs_train_list_levels[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WAcqG', 'Rv', 'zY#tE', 'tgU', 'tgU', 'tgU']\n",
      "['LnGi', 'LnGi', 'dZJa', 'Rv', 'Rv', 'ITZM']\n"
     ]
    }
   ],
   "source": [
    "# (ACT1-3) using input files compute the following\n",
    "docs_train = docs_train_list #list of documents where each document is a list of words\n",
    "docs_test = docs_test_list #list of documents where each document is a list of words\n",
    "labels = level  #list of labels each either -1 or +1 \n",
    "\n",
    "n_train = len(docs_train)\n",
    "n_test = len(docs_test)\n",
    "print(docs_train[40])\n",
    "print(docs_test[40])\n",
    "\n",
    "#CHECKS\n",
    "assert((n_train + n_test) == 1500)\n",
    "assert(np.sum(np.array(labels)) == -348)\n",
    "assert(len(docs_train[1])+len(docs_test[1]) == 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITZM', 'JgwBu', 'LnGi', 'OnJECH', 'Rv', 'WAcqG', 'dZJa', 'iWgN', 'tgU', 'zY#tE']\n",
      "{'ITZM': 0, 'JgwBu': 1, 'LnGi': 2, 'OnJECH': 3, 'Rv': 4, 'WAcqG': 5, 'dZJa': 6, 'iWgN': 7, 'tgU': 8, 'zY#tE': 9}\n"
     ]
    }
   ],
   "source": [
    "# (ACT4) design a function that takes list of documents (list of list of words) \n",
    "# as input and returns sorted list of distinct words \n",
    "# use built-in sort in python for sorting strings\n",
    "\n",
    "def make_vocabulary(docs):\n",
    "    unique_words = []\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            if docs[i][j] not in unique_words:\n",
    "                unique_words.append(docs[i][j])\n",
    "    unique_words.sort()\n",
    "    return unique_words\n",
    "\n",
    "vocab = make_vocabulary(docs_train)\n",
    "d = len(vocab) \n",
    "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
    "print(vocab)\n",
    "print(w2i)\n",
    "\n",
    "#CHECKS\n",
    "assert(vocab[2]==\"LnGi\")\n",
    "assert(vocab == sorted(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of X_train: \n",
      "[[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [3, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1], [1, 1, 0, 0, 0, 1, 0, 1, 3, 2, 1], [1, 0, 0, 1, 2, 0, 2, 0, 1, 0, 1]]\n",
      "Last 5 rows of X_test: \n",
      "[[0, 1, 1, 1, 1, 2, 0, 0, 2, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0, 3, 1, 1], [2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]]\n",
      "First 10 labels of training set:\n",
      "[-1, 1, -1, 1, -1, -1, 1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# (ACT5) design a function that takes \n",
    "# (1) docs: list of documents (i.e. list of list of words)\n",
    "# (2) w2i: a dictionary that maps words to index\n",
    "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
    "# (DO NOT forget last column of X which is all 1)\n",
    "\n",
    "def make_matrix(docs, w2i):\n",
    "    X = [[0 for i in range(len(w2i)+1)] for i in range(len(docs))]\n",
    "    for i in range(len(docs)):\n",
    "        X[i][-1] = 1\n",
    "        for k in w2i:\n",
    "            X[i][w2i[k]] = docs[i].count(k)\n",
    "    return X\n",
    "\n",
    "X_train = make_matrix(docs_train,w2i)\n",
    "X_test = make_matrix(docs_test,w2i)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# (ACT6-8)\n",
    "print (\"First 5 rows of X_train: \")\n",
    "print(X_train[:5])\n",
    "print (\"Last 5 rows of X_test: \")\n",
    "print(X_train[996:1000])\n",
    "print (\"First 10 labels of training set:\")\n",
    "print (labels[:10])\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(X_train)==6871)\n",
    "assert(np.sum(X_test)==3462)\n",
    "assert(np.sum(X_test[10] + X_train[10]) == 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       " [3, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       " [1, 1, 0, 0, 0, 1, 0, 1, 3, 2, 1],\n",
       " [1, 0, 0, 1, 2, 0, 2, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Predictor\n",
    "\n",
    "Let $\\mathbf{w} \\in \\mathbb{R}^{d+1}$, for a single document $\\mathbf{x} \\in \\mathbb{R}^{d+1}$, our predicted label is\n",
    "$$\n",
    "    \\mathsf{\\hat{y} = \\mathrm{sign}({\\mathbf{w} \\cdot \\mathbf{y}})}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "    \\displaystyle  \\mathrm{sign}(\\alpha) = \\begin{cases} \n",
    "      +1 & \\alpha \\geq 0 \\\\\n",
    "      -1 & \\alpha < 0 \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, for documents matrix $X \\in \\mathbb{R}^{n\\times(d+1)}$ we predict labels $\\hat{\\mathbf{y}} \\in \\{-1,+1\\}^n$\n",
    "$$\n",
    "    \\hat{\\mathbf{y}} = \\mathrm{sign}(X\\mathbf{w})\n",
    "$$\n",
    "where $\\mathrm{sign()}$ here is defined to elementwise applying the sign we defined previously.\n",
    "\n",
    "The error of our prediction over $X$ with true labels $\\mathbf{y}$, is defined to be\n",
    "$$\n",
    "    \\mathsf{\\mathrm{error}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{01}(y_i,\\hat{y}_i)}\n",
    "$$\n",
    "where $\\displaystyle \\mathsf{ \\ell_{01}(y,\\hat{y})= \\begin{cases} 1 & y\\neq\\hat{y} \\\\ 0 & \\text{otherwise}\\end{cases} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
    "# and computes the error\n",
    "def err(y,y_hat):\n",
    "    count = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_hat[i]:\n",
    "            count = count + 1\n",
    "    return count/len(y)\n",
    "\n",
    "#CHECKS\n",
    "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
    "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
    "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT10) Design a function that takes as input\n",
    "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
    "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
    "# and output\n",
    "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
    "\n",
    "def predict(X,w):\n",
    "    y_hat1 = []\n",
    "    for i in range(len(X)):\n",
    "        sum = 0\n",
    "        for j in range(len(w)):\n",
    "            sum = sum + X[i][j] * w[j]\n",
    "        if sum >= 0:\n",
    "            y_hat1.append(1)\n",
    "        else:\n",
    "            y_hat1.append(-1)\n",
    "    y_hat = np.array(y_hat1)\n",
    "    return y_hat\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1)) == n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate list for $\\mathbf{w}$\n",
    "we give you a small candidates list `candids` of $\\mathbf{w}$'s. We want you to find $\\mathbf{w}^*$ in this list which gives you the smallest error over **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of candidates lists : 10\n",
      "size of each w in candidates lists : 11\n",
      "[[ 0.24771893]\n",
      " [ 0.0797402 ]\n",
      " [ 0.06736309]\n",
      " [ 0.12221225]\n",
      " [ 0.00692924]\n",
      " [ 0.30678176]\n",
      " [ 0.1730867 ]\n",
      " [ 0.02923004]\n",
      " [ 0.33520645]\n",
      " [ 0.06869451]\n",
      " [-0.8189899 ]]\n"
     ]
    }
   ],
   "source": [
    "# Loading candidates list candids = [w0,w1,...]\n",
    "import pickle\n",
    "with open('candids.pkl', 'rb') as f:\n",
    "    candids = pickle.load(f)\n",
    "print(\"size of candidates lists :\", len(candids))\n",
    "print(\"size of each w in candidates lists :\", len(candids[0]))\n",
    "print(candids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of candidates:\n",
      "Candidate # 0 has error 0.252000\n",
      "Candidate # 1 has error 0.273000\n",
      "Candidate # 2 has error 0.000000\n",
      "Candidate # 3 has error 0.285000\n",
      "Candidate # 4 has error 0.290000\n",
      "Candidate # 5 has error 0.327000\n",
      "Candidate # 6 has error 0.180000\n",
      "Candidate # 7 has error 0.265000\n",
      "Candidate # 8 has error 0.223000\n",
      "Candidate # 9 has error 0.206000\n",
      "Index of best predictor: 2\n",
      "Best Predictor:\n",
      "[[ 0.22045993]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.11585252]\n",
      " [ 0.        ]\n",
      " [ 0.28635182]\n",
      " [ 0.14144608]\n",
      " [ 0.        ]\n",
      " [ 0.25414682]\n",
      " [ 0.        ]\n",
      " [-0.87828279]]\n"
     ]
    }
   ],
   "source": [
    "# (ACT11) fill err_list with training error of each candidate w\n",
    "err_list = []\n",
    "for i in range(len(candids)):\n",
    "    y_pred = predict(X_train, candids[i])\n",
    "    e = err(y_train, y_pred)\n",
    "    err_list.append(e)\n",
    "\n",
    "# (ACT12) index of w with smallest error over training set \n",
    "best_index = np.argmin(err_list)\n",
    "\n",
    "print(\"Training Error of candidates:\")\n",
    "for i,err in enumerate(err_list):\n",
    "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
    "\n",
    "print(\"Index of best predictor: %d\"%best_index)\n",
    "print(\"Best Predictor:\")\n",
    "print(candids[best_index])\n",
    "\n",
    "#CHECKS\n",
    "assert(np.sum(err_list)<=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Best predictor\n",
    "w_best = candids[best_index]\n",
    "\n",
    "# (ACT13) Use w_best to predict labels for X_test \n",
    "y_test = predict(X_test, candids[best_index])\n",
    "\n",
    "# (ACT14) print first 10 labels predicted for test set\n",
    "print(y_test[:10])\n",
    "\n",
    "#CHECKS\n",
    "def my_hash(y):\n",
    "    p1 = 28433\n",
    "    p2 = 577\n",
    "    ret = 0\n",
    "    for e in range(len(y)):\n",
    "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
    "    return ret\n",
    "assert(my_hash(y_test) == 19262)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ACT15) using X_train and X_test\n",
    "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
    "# using two subplots (bar plots) one for X_train and one for X_test\n",
    "# you might find plt.bar useful\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACT15\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
